[TOC]

# 数据存储部分


### `Node.java`

> 存储节点相关信息

- ```int ```
    - degree  //节点的度
    - clusterNo //节点所属集群
- ```String```
    - name //实体名称
    - property  //节点属性 （高密、边界、保护、噪声）
- ```List<String>```
    - List  //存储与该节点相关的SPO信息。 使用List的原因是：单个节点的spo不会重复。
- `bool`
    - visited //标志是否访问过
    - isBorder //标志是否是边界节点



### `Cluster.java`

- `int`

    - clusterNo  //集群号
    - density //集群密度（集群内节点的总度数之和）
    - nodeNum //集群内的节点个数

- `HashSet<String>`

    - spo //存储集群内的所有节点的spo信息 ，会出现重复所以用了HashSet

### `Config.java`

> 这个类用于 1.分配资源  2.参数配置(密度阈值等)  3. 存储路径设置

- `HashMap<String,Node>` // 存储实体名字与对应Node间的映射
    - 避免重复存储，所以用Hash
    - 便于根据name寻找Node（架设AB节点相连，已知节点A，就可以得到与A相关的SPO，从而得到B的name，然后使用map映射找到对应的name的Node）
- `HashMap<Node,HashSet<Cluster>> ` // 存储索引
    - value使用HashSet避免重复
- `List<Cluster>` 存储集群， 下标0,1,2,3 分别对应集群0,1,2,3
- `List<HashSet<Node>>` 存储集群内的节点，为了避免重复保存，使用了HashSet ， 下标0,1,2,3 分别对应集群0,1,2,3



-------------------------------------------------

# 数据导入部分

### `Input_Output.java`

- read_spo()  
    - 读取存储三元组的文件.CSV 到 `ArrayList<String>`

###`Put_spo_in_hashmap.java` 

1. 读 `ArrayList<String>` 中的每行，按 "," 拆分
2. 分出S、P、O信息， 主要处理S O 包含的实体名： 在存储节点映射的`HashMap<String,Node>`中检索是否已经存在这个实体。
    - 若存在： get这一实体名对应的的Node，将Node的degree + 1；
    - 若不存在：1. new一个Node   2. 将本条SPO加入Node的`List<String>`中 3. 将`<String,Node> ` 加入`HashMap<String,Node>`

------------------------------------------

# 排序部分

### `HashMap_Sort.java`

将HashMap中的每一条加入List，利用List的Sort功能排序。 

- 要Override原始的Sort，将排序依据改为Node的degree

-----------------------------------------------

# 数据导出部分

### `Input_Output.java`

- `writeAllNodeCSV()` //导出`HashMap<String,Node>`中的所有Node
- `writeClusterNodeCSV()`
    - `writeOneClusterNode() ` // 导出`List<HashSet<Node>>`中所有Node，按照下标分别存储到不同Cluster对应的文件夹
- `writeClusterInfo() ` //导出每个集群的信息：集群号 / 密度 / 节点数量  `List<CLuster>` 
- `writeClusterSpo()`  // 导出每个集群所包含的三元组 `List<CLuster>` 

- `writeIndexTable()` //导出索引表 `HashMap<String,HashSet<Cluster>>`

---------------------------------------------

# 聚类部分（核心

### `CliqueCluster.java`

- `do_clique()` （缩进为函数依赖关系）（以下为聚类的执行流程）
    1. `initialize()` ： 初始化： new 噪声节点、边界节点的List ，以及用于标志位复位的LIst；
    2. `search_all_hash()`  :第一次遍历
        1. `bfs()`;  //从高密节点开始的BFS
            1. `get_another()`;  //已知SPO和其中一个实体A，寻找另一个实体
    3. `deal_with_noise()`:  //处理噪声节点
        1. `collect_noise()`;  //将所有噪声节点加入List
        2. `bfs_noist()`   //从噪声节点开始的BFS
    4. deal_with_border ; //处理边界节点
        1. deal_one_border_node(); //处理一个边界节点
    5. 输出信息

#### `search_all_hash()`

1. 建立一个噪声集群`CLuster 0 ` ，以及存储该集群节点的`HashSet<Node>`
2. 遍历实体的`HashMap<String,Node>` ， 针对每个Node：
    - 是否被访问过，若是，则无操作。 若非：
        - 节点的度是否达到阈值
            - 达到： 当前集群号加1`clusterNo++ `，表示新的集群出现了，然后以此节点开始进行BFS循环
            - 未达到：暂时标为噪声， 不标记为访问过。 等到后续处理。



##### `bfs()`

1. 常规BFS框架

    1. 建立队列
    2. 头结点入队
    3. 进入while循环 ，循环结束条件是队列为空
    4. 队头出队，处理头节点，入队头结点周围一跳范围的节点
2. 每次bfs都代表新的cluster开始 ； 但是要避免重复new `Cluster`和`HashSet<Node>`.

    1. 先对存储Cluster和  Cluster内的Node  的list以当前的clusterNo作为下标使用get操作
    2. 若出现了异常，则表示这个集群还没有创建，new一个集群new一个HashSet
    3. 若没有异常，直接用get到的集群和HashSet操作当前节点即可。
3. while循环内，对头结点的处理

    1. 依据入队要求（下面会说到），只有高密度节点才能入队，其余类型都只做标记，不进一步处理。

    2. 高密度节点处理方式：

        - 针对Node本身
			1. 	`setVisited`  //访问过
			2. `setClusterNo` //给集群号
			3. 	`setPropertyDen ` //节点类型
		- 针对所在的Cluster
		    1. `clu.addDensity`  //集群密度增加
		    2. `nod.add`  //加到集群的节点列表
4. while循环内，入队的操作
    1. `top.getList() ` 得到与它相关的spo， 从而通过`get_another(spo,top.getName())` 方法，找到与当前节点相连的所有节点
    2. 针对每个节点处理：
        1. 判断是否`isVisited()`
        2. 若非： 
            1. 若度为1  （保护节点）
            	1. `setVisited`
				2. `setClusterNo`
				3. `setPropertyPro `
				4. `clu.addDensity` 
				5. `nod.add`
			1. 若度大于1小于阈值  （边界节点）
                1. `setVisited`
            	2. `setClusterNo`
				3. `setPropertyBorder `
				4. `clu.addDensity` 
				 5. `nod.add`
				 6. `border_Node.add()` //添加到边界节点List
			1. 若度大于阈值 （高密节点）
			    1. 入队
		3. 若是
		    - 这个节点一定是边界点，后续处理
		        - 一定是其他Cluster的边界点，已经存在border_node列表中了
		        - 另一个集群BFS时，因为度大于1小于阈值，被标记成了边界，没有进一步访问。
5. BFS的while循环结束后
    1. clu加入 `List<Cluster>`
    2. nod加入 `List<HashSet<Node>>`

#### `deal_with_noise()`

1. `collect_noise()`
    - 遍历`HashMap` 将疑似噪声的点加入`List<Node> ` ，生成噪声处理表
    - 疑似噪声的点： 1.未被遍历到的点 2。主动设集群号为0的点  ；  他们的共有特性为：`Node.clusterNo==0`
2. 遍历噪声点处理列表，对每个点执行`bfs_noise()` ，返回值是距离他们最近的非噪声节点的集群号，若没有非噪声节点则返回值是0。
    1. 若返回值是0： 这个节点是孤岛节点，为真噪声
    2. 若非0 ： 伪噪声节点， 将其从List中remove， 并加入border_Node的List
3. 为这些节点设置clusterNo 和 setVisited （这样，所有的节点都是visited了 ）

##### `bfs_noise()`

1. 常规BFS
2. while中，对队首元素的处理：
    1. 反向标记 （因为当前所有点都是visited， 反向标记为unvisited就可以区分访问过和未访问过）
    2. 被标记的Node加入List（方便在噪声BFS完成后给这些node的标记复位）
    3. 若队首的clusterNo非0，直接返回这个集群号
3. while中，入队操作
    - 遍历队首的spo，`get_another()`后，若没有被反向标记（避免有环，重复入队造成死循环），则直接入队。
4. BFS结束（队空了），返回0， 表示是真噪声节点。



#### `deal_with_border()`



1. 遍历`border_Node`列表，处理所有意思边界节点
2. 使用`deal_one_border_node()`方法判断是否是真的边界
3. 若不是，则remove； 



##### `deal_one_border_node()`

1. 初始化操作：获取当前节点的`clusterNo`  ； 初始化标志位`is_border`  ；建立存储当前节点所在集群以及与其相连的节点集群号的`HashSet<Cluster>` （避免重复添加集群号）

2. 获取当前节点集群号，入Set ； 使用`Node.getList` 和 `get_another()`方法获取与其相连的所有节点，开始遍历：

3. 对于每一个相连接点： 

    - 如果他的集群号与当前集群号不同：
        - 是边缘结点
        -  is_border=true 
        -   `HashSet<Cluster>`添加该节点的集群号。
        - 当前节点所在集群中添加相连节点的Node信息。（为跨区查询方便，边缘结点在两个集群中都存储）

    - 若相同：
        - 不作处理。

4. 遍历完成后，判断是否需要切割（? is_border ==true ）

    - 若false ： 
        - 从边缘结点列表中remove当前节点
        - 返回false
    - 若true：
        - `setBorder()`  //因为有一些伪噪声节点也参与了处理，他们的property现在还是noise，如果是真边界，要改一下。
        - 添加到索引表: `Index_table.put(start.getName(),linked_clusters);`
        - 返回true

------------------------------------------